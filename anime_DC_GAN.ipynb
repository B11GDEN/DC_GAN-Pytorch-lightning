{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pytorch_lightning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data import sampler\nfrom torch.optim import SGD, Adam\n\nimport torchvision.models as models\n\nfrom sklearn.model_selection import train_test_split\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport PIL\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimage_name_list = []\nfor dirname, _, filenames in os.walk('/kaggle/input/animed/cropped'):\n    for filename in filenames:\n        image_name_list.append(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size = 400\nw = 80\n\nimages = []\ni = 0\nj = 0\nwith tqdm(total=size, unit='img') as pbar:\n    while i < size:\n        j += 1\n        try:\n            image = Image.open(image_name_list[j])\n        except IOError:\n            continue\n            \n        image = image.resize((w, w))\n        image = np.array(image)\n        image = image / 255\n        image = image.transpose([2, 0, 1])\n        images.append(image)\n        i += 1\n        pbar.update(1)\n        \ntrain_dataset = TensorDataset(torch.tensor(np.array(images)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Flatten(nn.Module):\n    \n    def forward(self, x):\n        N, C, H, W = x.size()\n        return x.view(N, -1)  \n    \nclass Unflatten(nn.Module):\n    \n    def __init__(self, N=-1, C=128, H=7, W=7):\n        super(Unflatten, self).__init__()\n        self.N = N\n        self.C = C\n        self.H = H\n        self.W = W\n    def forward(self, x):\n        return x.view(self.N, self.C, self.H, self.W)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch         = 1000\nbatch_size    = 100\nnoise_size    = 296\nlr_gen        = 1e-3\nlr_dis        = 1e-3\nverbose       = 100\ndtype         = torch.cuda.FloatTensor\n\nclass AnimeModel(pl.LightningModule):\n\n    def __init__(self):\n        \n        super(AnimeModel, self).__init__()\n        \n        self.D = nn.Sequential(\n            \n            nn.Conv2d(3, 32, kernel_size = 5, stride = 1),\n            nn.LeakyReLU(inplace=True),\n            nn.MaxPool2d(2,2),\n            nn.Conv2d(32, 64,kernel_size = 5, stride = 1),\n            nn.LeakyReLU(inplace=True),\n            nn.MaxPool2d(2,2),\n            Flatten(),\n            nn.Linear(18496, 1024),\n            nn.LeakyReLU(inplace=True),\n            nn.Linear(1024,1)\n            \n        ).type(dtype)\n        \n        self.G = nn.Sequential(\n            \n            nn.Linear(noise_size,1024),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(1024),\n            nn.Linear(1024, 8*w*w),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(8*w*w),\n            Unflatten(batch_size, 128, w // 4, w // 4),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(64),\n            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n            nn.Tanh(),\n            \n        ).type(dtype)\n        \n        self.iter = 0\n\n    def training_step(self, batch, batch_nb, optimizer_idx):\n        \n        self.iter += 1\n        \n        if self.iter % verbose == 0:\n            \n            real_images, = batch\n            \n            g_fake_seed     = self.sample_noise(batch_size, noise_size).type(dtype)\n            fake_images     = self.G(g_fake_seed)\n            \n            fig = plt.figure()\n            plt.imshow(fake_images[0].permute(1, 2, 0).cpu().detach())\n            plt.show()\n            \n            self.logger.experiment.add_image(\"fake_image0\", fake_images[0])\n            self.logger.experiment.add_image(\"fake_image1\", fake_images[1])\n            self.logger.experiment.add_image(\"fake_image2\", fake_images[2])\n            self.logger.experiment.add_image(\"fake_image3\", fake_images[3])\n            \n            self.logger.experiment.add_image(\"real_image0\", real_images[0])\n            self.logger.experiment.add_image(\"real_image1\", real_images[1])\n            self.logger.experiment.add_image(\"real_image2\", real_images[2])\n            self.logger.experiment.add_image(\"real_image3\", real_images[3])\n        \n        if optimizer_idx == 0:\n            \n            g_fake_seed     = self.sample_noise(batch_size, noise_size)\n            fake_images     = self.G(g_fake_seed)\n\n            gen_logits_fake = self.D(fake_images)\n            g_error         = self.generator_loss(gen_logits_fake)\n            \n            return {\n                \"loss\"         : g_error,\n                'progress_bar' : {'gen_loss': g_error},\n                'log'          : {'gen_loss': g_error}\n            }\n            \n        \n        if optimizer_idx == 1:\n            \n            X,              = batch\n            real_data       = X.type(dtype)\n            logits_real     = self.D(2* (real_data - 0.5))\n        \n            g_fake_seed     = self.sample_noise(batch_size, noise_size)\n            fake_images     = self.G(g_fake_seed).detach()\n            logits_fake     = self.D(fake_images)\n\n            d_total_error   = self.discriminator_loss(logits_real, logits_fake)\n            \n            return {\n                \"loss\"         : d_total_error,\n                'progress_bar' : {'disc_loss': d_total_error},\n                'log'          : {'disc_loss': d_total_error}\n            }\n   \n\n    def configure_optimizers(self):\n        generator_opt    = Adam(self.G.parameters(), lr = lr_gen,  betas=(0.5, 0.999))\n        disriminator_opt = Adam(self.D.parameters(), lr = lr_dis,  betas=(0.5, 0.999))\n        return generator_opt, disriminator_opt\n\n    def sample_noise(self, batch_size, dim):\n        return (torch.rand([batch_size, dim])*2 - 1).type(dtype)\n    \n    def bce_loss(self, input, target):\n        neg_abs = - input.abs()\n        loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n        return loss.mean()\n    \n    def discriminator_loss(self, logits_real, logits_fake):\n        N = logits_real.shape[0]\n        \n        true_labels  = torch.ones(N).type(dtype)\n        false_labels = torch.zeros(N).type(dtype)\n        \n        loss   = self.bce_loss(logits_real, true_labels) + self.bce_loss(logits_fake, false_labels)\n        \n        return loss\n    \n    def generator_loss(self, logits_fake):\n        N = logits_fake.shape[0]\n        \n        true_labels = torch.ones(N).type(dtype)\n        \n        loss = self.bce_loss(logits_fake, true_labels)\n        return loss\n    \n    @pl.data_loader\n    def train_dataloader(self):\n        return DataLoader(train_dataset, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = AnimeModel()\ntrainer = Trainer(early_stop_callback=False, max_nb_epochs=epoch)\ntrainer.fit(model)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}